import sys
import time
from pathlib import Path

# Imports de vos modules
from scraper.e_commerce_scraper import EcommerceScraper
from scraper.bource_scraper import BourseScraper
from scraper.news_scraper import NewsScraper
from utils.exporter import export_data
from utils.cleaner import DataCleaner
from utils.robot_check import is_scraping_allowed

class ScrapingManager:
    """Gestionnaire principal pour le scraping avec options avancÃ©es"""
    
    def __init__(self):
        self.scrapers = {
            "ecommerce": EcommerceScraper,
            "bourse": BourseScraper,
            "news": NewsScraper
        }
        self.formats_supportes = ["csv", "json", "xlsx", "pdf"]
    
    def choisir_scraper(self, type_site, url):
        """Factory pattern pour crÃ©er le bon scraper"""
        if type_site not in self.scrapers:
            raise ValueError(f"Type de site '{type_site}' non reconnu. Types disponibles : {list(self.scrapers.keys())}")
        
        # DÃ©tecter des sites spÃ©cifiques pour utiliser des scrapers optimisÃ©s
        if type_site == "bourse":
            if "yahoo" in url.lower():
                from scraper.bource_scraper import YahooFinanceScraper
                return YahooFinanceScraper(url)
            elif "bloomberg" in url.lower():
                from scraper.bource_scraper import BloombergScraper
                return BloombergScraper(url)
            elif "marketwatch" in url.lower():
                from scraper.bource_scraper import MarketwatchScraper
                return MarketwatchScraper(url)
        
        return self.scrapers[type_site](url)
    
    def verifier_robots_txt(self, url, force=False):
        """VÃ©rifie robots.txt avec option de forÃ§age"""
        if force:
            print("âš ï¸ VÃ©rification robots.txt ignorÃ©e (mode forcÃ©)")
            return True
            
        try:
            if not is_scraping_allowed(url):
                print("ðŸš« Le scraping de ce site n'est pas autorisÃ© selon robots.txt")
                
                choix = input("Voulez-vous continuer quand mÃªme ? (y/N): ").strip().lower()
                if choix in ['y', 'yes', 'oui', 'o']:
                    print("âš ï¸ Scraping en mode non-conforme Ã  robots.txt")
                    return True
                else:
                    return False
            else:
                print("âœ… Scraping autorisÃ© par robots.txt")
                return True
                
        except Exception as e:
            print(f"âš ï¸ Impossible de vÃ©rifier robots.txt ({e}). Continuation...")
            return True
    
    def scraper_avec_options(self, scraper, options=None):
        """Lance le scraping avec des options avancÃ©es"""
        if options is None:
            options = {}
        
        print("ðŸš€ DÃ©but du scraping...")
        start_time = time.time()
        
        try:
            # Options de scraping avancÃ©es
            if options.get('stealth_mode', False):
                print("ðŸ¥· Mode furtif activÃ©")
                scraper.enable_stealth_mode()
            
            if options.get('delay', 0) > 0:
                print(f"â±ï¸ DÃ©lai entre requÃªtes : {options['delay']}s")
                scraper.set_delay(options['delay'])
            
            if options.get('user_agent'):
                print(f"ðŸ”§ User-Agent personnalisÃ© : {options['user_agent'][:50]}...")
                scraper.set_user_agent(options['user_agent'])
            
            # Lancement du scraping
            data = scraper.scrape()
            
            elapsed_time = time.time() - start_time
            print(f"âœ… Scraping terminÃ© en {elapsed_time:.2f}s")
            
            return data
            
        except Exception as e:
            print(f"âŒ Erreur lors du scraping : {e}")
            return None
    
    def nettoyer_donnees(self, data, site_type, options_nettoyage=None):
        """Nettoie les donnÃ©es avec options personnalisables"""
        if not data:
            return data
        
        print("ðŸ§¹ Nettoyage des donnÃ©es...")
        
        try:
            cleaner = DataCleaner(site_type=site_type)
            
            if options_nettoyage:
                cleaner.configure(options_nettoyage)
            
            cleaned_data = cleaner.clean(data)
            print(f"âœ… {len(cleaned_data)} Ã©lÃ©ments nettoyÃ©s")
            
            return cleaned_data
            
        except Exception as e:
            print(f"âš ï¸ Erreur lors du nettoyage : {e}")
            return data  # Retourner les donnÃ©es brutes en cas d'erreur
    
    def exporter_donnees(self, data, filename, format_choisi, options_export=None):
        """Exporte les donnÃ©es avec gestion d'erreurs amÃ©liorÃ©e"""
        if not data:
            print("âŒ Aucune donnÃ©e Ã  exporter")
            return False
        
        if format_choisi not in self.formats_supportes:
            print(f"âŒ Format '{format_choisi}' non supportÃ©. Formats disponibles : {self.formats_supportes}")
            return False
        
        try:
            # CrÃ©er le dossier de sortie si nÃ©cessaire
            output_dir = Path("output")
            output_dir.mkdir(exist_ok=True)
            
            filepath = output_dir / f"{filename}.{format_choisi}"
            
            export_data(data, str(filepath.stem), format_choisi, options_export)
            
            print(f"âœ… DonnÃ©es exportÃ©es : {filepath}")
            print(f"ðŸ“Š {len(data)} Ã©lÃ©ments exportÃ©s")
            
            return True
            
        except Exception as e:
            print(f"âŒ Erreur lors de l'export : {e}")
            return False

def interface_utilisateur():
    """Interface utilisateur interactive"""
    manager = ScrapingManager()
    
    print("=" * 60)
    print("ðŸ•·ï¸  SCRAPER UNIVERSEL")
    print("=" * 60)
    
    # Configuration du scraping
    print("\nðŸ“‹ CONFIGURATION")
    print(f"Types de sites : {' | '.join(manager.scrapers.keys())}")
    type_site = input("Type de site Ã  scraper : ").strip().lower()
    
    if type_site not in manager.scrapers:
        print(f"âŒ Type '{type_site}' non reconnu")
        return
    
    url = input("URL du site : ").strip()
    if not url:
        print("âŒ URL requise")
        return
    
    # Options avancÃ©es
    print("\nâš™ï¸ OPTIONS AVANCÃ‰ES (optionnel)")
    
    force_scraping = input("Ignorer robots.txt ? (y/N): ").strip().lower() in ['y', 'yes', 'oui', 'o']
    
    stealth_mode = input("Mode furtif ? (y/N): ").strip().lower() in ['y', 'yes', 'oui', 'o']
    
    try:
        delay = float(input("DÃ©lai entre requÃªtes (secondes, 0 par dÃ©faut): ") or "0")
    except ValueError:
        delay = 0
    
    nettoyer = input("Nettoyer les donnÃ©es ? (Y/n): ").strip().lower() not in ['n', 'no', 'non']
    
    # Configuration export
    print(f"\nðŸ“¤ EXPORT")
    print(f"Formats disponibles : {' | '.join(manager.formats_supportes)}")
    format_choisi = input("Format de sortie : ").strip().lower()
    filename = input("Nom du fichier (sans extension) : ").strip()
    
    if not filename:
        filename = f"scraping_{type_site}_{int(time.time())}"
    
    # VÃ©rification robots.txt
    if not manager.verifier_robots_txt(url, force=force_scraping):
        print("ðŸ›‘ Scraping annulÃ©")
        return
    
    # Configuration des options
    options_scraping = {
        'stealth_mode': stealth_mode,
        'delay': delay,
        'user_agent': None  # Peut Ãªtre Ã©tendu
    }
    
    # ExÃ©cution du scraping
    print("\n" + "=" * 60)
    
    try:
        scraper = manager.choisir_scraper(type_site, url)
        data = manager.scraper_avec_options(scraper, options_scraping)
        
        if not data:
            print("âŒ Aucune donnÃ©e rÃ©cupÃ©rÃ©e")
            return
        
        # Nettoyage optionnel
        if nettoyer:
            data = manager.nettoyer_donnees(data, type_site)
        
        # Export
        success = manager.exporter_donnees(data, filename, format_choisi)
        
        if success:
            print("\nðŸŽ‰ Scraping terminÃ© avec succÃ¨s !")
        else:
            print("\nâŒ Ã‰chec de l'export")
            
    except Exception as e:
        print(f"\nðŸ’¥ Erreur fatale : {e}")

def mode_commande():
    """Mode ligne de commande pour l'automatisation"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Scraper universel")
    parser.add_argument("type", choices=["ecommerce", "bourse", "news"], help="Type de site")
    parser.add_argument("url", help="URL Ã  scraper")
    parser.add_argument("-o", "--output", default="output", help="Nom du fichier de sortie")
    parser.add_argument("-f", "--format", choices=["csv", "json", "xlsx", "pdf"], default="json", help="Format de sortie")
    parser.add_argument("--force", action="store_true", help="Ignorer robots.txt")
    parser.add_argument("--stealth", action="store_true", help="Mode furtif")
    parser.add_argument("--delay", type=float, default=0, help="DÃ©lai entre requÃªtes")
    parser.add_argument("--no-clean", action="store_true", help="Ne pas nettoyer les donnÃ©es")
    
    args = parser.parse_args()
    
    manager = ScrapingManager()
    
    # VÃ©rification robots.txt
    if not manager.verifier_robots_txt(args.url, force=args.force):
        return
    
    # Options
    options_scraping = {
        'stealth_mode': args.stealth,
        'delay': args.delay
    }
    
    try:
        scraper = manager.choisir_scraper(args.type, args.url)
        data = manager.scraper_avec_options(scraper, options_scraping)
        
        if data and not args.no_clean:
            data = manager.nettoyer_donnees(data, args.type)
        
        if data:
            manager.exporter_donnees(data, args.output, args.format)
    
    except Exception as e:
        print(f"Erreur : {e}")
        sys.exit(1)

def main():
    """Point d'entrÃ©e principal"""
    if len(sys.argv) > 1:
        # Mode ligne de commande
        mode_commande()
    else:
        # Mode interactif
        interface_utilisateur()

if __name__ == "__main__":
    main()