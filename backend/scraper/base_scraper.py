# scraper/base_scraper.py

import urllib.request
import urllib.parse
from urllib.error import URLError, HTTPError
from bs4 import BeautifulSoup
import time
import random
import requests
from fake_useragent import UserAgent

class BaseScraper:
    def __init__(self, site_url, user_agent=None):
        self.site_url = site_url
        self.base_user_agent = user_agent or 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        self.current_user_agent = self.base_user_agent
        
        # Options pour le mode furtif
        self.stealth_mode = False
        self.delay = 0
        self.session = requests.Session()
        self.ua_generator = None
        
        # Initialiser fake_useragent si disponible
        try:
            self.ua_generator = UserAgent()
        except:
            self.ua_generator = None
        
        # Headers par d√©faut pour para√Ætre plus humain
        self.default_headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
    
    def enable_stealth_mode(self):
        """Active le mode furtif"""
        self.stealth_mode = True
        print("ü•∑ Mode furtif activ√©")
        
        # Utiliser une session pour maintenir les cookies
        self.session.headers.update(self.default_headers)
        
        # Ajouter un referer r√©aliste
        parsed_url = urllib.parse.urlparse(self.site_url)
        self.session.headers['Referer'] = f"https://www.google.com/search?q={parsed_url.netloc}"
    
    def set_delay(self, delay):
        """D√©finit le d√©lai entre les requ√™tes"""
        self.delay = delay
        print(f"‚è±Ô∏è D√©lai configur√© : {delay}s")
    
    def set_user_agent(self, user_agent):
        """D√©finit un User-Agent personnalis√©"""
        self.current_user_agent = user_agent
        self.session.headers['User-Agent'] = user_agent
        print(f"üîß User-Agent configur√©")
    
    def get_random_user_agent(self):
        """G√©n√®re un User-Agent al√©atoire"""
        if self.ua_generator and self.stealth_mode:
            try:
                return self.ua_generator.random
            except:
                pass
        
        # Fallback avec une liste pr√©d√©finie
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0'
        ]
        return random.choice(user_agents)
    
    def apply_delay(self):
        """Applique un d√©lai avant la requ√™te"""
        if self.delay > 0:
            if self.stealth_mode:
                # D√©lai al√©atoire en mode furtif
                actual_delay = self.delay + random.uniform(-0.5, 1.0)
                actual_delay = max(0.1, actual_delay)  # Minimum 0.1s
            else:
                actual_delay = self.delay
            
            time.sleep(actual_delay)
    
    def get_html_urllib(self):
        """M√©thode originale avec urllib (fallback)"""
        try:
            headers = {'User-Agent': self.current_user_agent}
            req = urllib.request.Request(self.site_url, headers=headers)
            
            with urllib.request.urlopen(req, timeout=30) as response:
                return response.read()
                
        except HTTPError as e:
            print(f"[HTTPError] {e.code} - {e.reason}")
        except URLError as e:
            print(f"[URLError] {e.reason}")
        except Exception as e:
            print(f"[Exception] {str(e)}")
        return None
    
    def get_html_requests(self):
        """M√©thode am√©lior√©e avec requests et mode furtif"""
        try:
            # Appliquer le d√©lai
            self.apply_delay()
            
            # Headers dynamiques en mode furtif
            if self.stealth_mode:
                self.session.headers['User-Agent'] = self.get_random_user_agent()
            else:
                self.session.headers['User-Agent'] = self.current_user_agent
            
            # Faire la requ√™te
            response = self.session.get(
                self.site_url,
                timeout=30,
                allow_redirects=True
            )
            
            response.raise_for_status()  # L√®ve une exception pour les codes d'erreur HTTP
            
            return response.content
            
        except requests.exceptions.HTTPError as e:
            print(f"[HTTPError] {e.response.status_code} - {e}")
        except requests.exceptions.ConnectionError as e:
            print(f"[ConnectionError] {e}")
        except requests.exceptions.Timeout as e:
            print(f"[Timeout] {e}")
        except requests.exceptions.RequestException as e:
            print(f"[RequestException] {e}")
        except Exception as e:
            print(f"[Exception] {str(e)}")
        return None
    
    def get_html(self):
        """Point d'entr√©e principal pour r√©cup√©rer le HTML"""
        if self.stealth_mode:
            return self.get_html_requests()
        else:
            # Essayer d'abord avec requests, fallback sur urllib
            html = self.get_html_requests()
            if html is None:
                print("üîÑ Tentative avec urllib...")
                html = self.get_html_urllib()
            return html
    
    def get_soup(self):
        """R√©cup√®re et parse le HTML avec BeautifulSoup"""
        html = self.get_html()
        if html:
            try:
                return BeautifulSoup(html, 'html.parser')
            except Exception as e:
                print(f"[BeautifulSoup Error] {e}")
                return None
        return None
    
    def scrape(self):
        """M√©thode abstraite √† impl√©menter dans les classes filles"""
        raise NotImplementedError("La m√©thode `scrape` doit √™tre d√©finie dans la classe fille.")
    
    def test_connection(self):
        """Teste la connexion au site"""
        print(f"üîç Test de connexion √† {self.site_url}")
        
        html = self.get_html()
        if html:
            print(f"‚úÖ Connexion r√©ussie ({len(html)} bytes r√©cup√©r√©s)")
            return True
        else:
            print("‚ùå √âchec de la connexion")
            return False
    
    def get_response_info(self):
        """R√©cup√®re des informations sur la r√©ponse (en mode requests)"""
        if hasattr(self.session, 'cookies') and self.session.cookies:
            print(f"üç™ Cookies re√ßus : {len(self.session.cookies)}")
        
        # Faire une requ√™te HEAD pour obtenir les headers
        try:
            response = self.session.head(self.site_url, timeout=10)
            print(f"üìä Status: {response.status_code}")
            print(f"üìã Headers re√ßus : {len(response.headers)}")
            
            # Headers int√©ressants
            interesting_headers = ['server', 'x-powered-by', 'cloudflare-ray-id', 'cf-ray']
            for header in interesting_headers:
                if header in response.headers:
                    print(f"   {header}: {response.headers[header]}")
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Impossible d'obtenir les infos de r√©ponse : {e}")

    def save_html_debug(self, filename="debug.html"):
        """Sauvegarde le HTML pour d√©bogage"""
        html = self.get_html()
        if html:
            with open(filename, 'wb') as f:
                f.write(html)
            print(f"üíæ HTML sauvegard√© dans {filename}")
        else:
            print("‚ùå Aucun HTML √† sauvegarder")